{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "# Import numpy, pandas, matpltlib.pyplot, sklearn modules and seaborn\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\npd.set_option('display.max_rows', 200)\npd.set_option('display.max_columns', 200)\nplt.style.use('ggplot')\n\n\n\n# Import LogisticRegression\nfrom sklearn.linear_model import LogisticRegression\n\n# Import KNeighborsClassifier from sklearn.neighbors\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_curve, auc\n\ndf = pd.read_csv('US_Accidents_Dec21_updated.csv')\ndf.info()\nState = df.State.value_counts().reset_index()\nState.columns = ['State','Accidents']\nState.head()\nState['Percentage'] = round(State['Accidents'] * 100 / State['Accidents'].sum() , 2)\nState.head()\nplt.figure(figsize=(18,9))\ngraph = plt.bar(State.State.head(10),State.Accidents.head(10),\n               color=('#ed1b0c','#ed6a0c','#edcf0c','#93ed0c','#0ced5f',\n                      '#0cedaa','#0cb5ed','#0c6aed','#390ced','#ed0ce2'))\nplt.title('Percentage of accidents occured across the top 10 States',ha='center',weight='bold')\nplt.xlabel(\"State\",ha='center',weight='bold')\nplt.ylabel(\"Number of accidents\",ha='center',weight='bold')\n \n\nplt.show()\n# Convert Start_Time and End_Time to datetypes\ndf['Start_Time'] = pd.to_datetime(df['Start_Time'], errors='coerce')\ndf['End_Time'] = pd.to_datetime(df['End_Time'], errors='coerce')\n\n# Extract year, month, day, hour and weekday\ndf['Year']=df['Start_Time'].dt.year\ndf['Month']=df['Start_Time'].dt.strftime('%b')\ndf['Day']=df['Start_Time'].dt.day\ndf['Hour']=df['Start_Time'].dt.hour\ndf['Weekday']=df['Start_Time'].dt.strftime('%a')\n\n# Extract the amount of time in the unit of minutes for each accident, round to the nearest integer\ntd='Time_Duration(min)'\ndf[td]=round((df['End_Time']-df['Start_Time'])/np.timedelta64(1,'m'))\ndf.info()\n# Check if there is any outliers\ndf[td][df[td]<=0]\n# Remove outliers for Time_Duration(min): n * standard_deviation (n=3), backfill with median\n\nn=3\n\nmedian = df[td].median()\nstd = df[td].std()\noutliers = (df[td] - median).abs() > std*n\n\n# Set outliers to NAN\ndf[outliers] = np.nan\n\n# Fill NAN with median\ndf[td].fillna(median, inplace=True)\n\ndf.info()\n# Print time_duration information\nprint('Max time to clear an accident: {} minutes or {} hours or {} days; Min to clear an accident td: {} minutes.'.format(df[td].max(),round(df[td].max()/60), round(df[td].max()/60/24), df[td].min()))\n# Set the list of features to include in Machine Learning\nfeature_lst=['Severity','Start_Lng',\n             'Start_Lat','Distance(mi)','Side','City',\n             'County','State','Timezone','Temperature(F)',\n             'Humidity(%)','Pressure(in)', 'Visibility(mi)', \n             'Wind_Direction','Weather_Condition','Amenity',\n             'Bump','Crossing','Give_Way','Junction','No_Exit',\n             'Railway','Roundabout','Station','Stop','Traffic_Calming',\n             'Traffic_Signal','Turning_Loop','Sunrise_Sunset','Hour','Weekday', 'Time_Duration(min)']\n# Select the dataset to include only the selected features\ndf_sel=df[feature_lst].copy()\ndf_sel.info()\n# Check missing values\ndf_sel.isnull().mean()\ndf_sel.dropna(subset=df_sel.columns[df_sel.isnull().mean()!=0], how='any', axis=0, inplace=True)\ndf_sel.shape\n# Set state\nstate='CA'\n\n# Select the state of California\ndf_state=df_sel.loc[df_sel.State==state].copy()\ndf_state.drop('State',axis=1, inplace=True)\ndf_state.info()\n# Map of accidents, color code by county\n\nsns.scatterplot(x='Start_Lng', y='Start_Lat', data=df_state, hue='County', legend=False, s=20)\nplt.show()\n# Set county\ncounty='San Francisco'\n\n# Select the state of CA\ndf_county=df_state.loc[df_state.County==county].copy()\ndf_county.drop('County',axis=1, inplace=True)\ndf_county.info()\n# Map of accidents, color code by city\n\nsns.scatterplot(x='Start_Lng', y='Start_Lat', data=df_county, hue='City', legend=False, s=20)\nplt.show()\n# Generate dummies for categorical data\ndf_county_dummy = pd.get_dummies(df_county,drop_first=True)\n\ndf_county_dummy.info()\n# Assign the data\ndf=df_county_dummy\n\n# Set the target for the prediction\ntarget='Severity'\n\n\n# Create arrays for the features and the response variable\n\n# set X and y\ny = df[target]\nX = df.drop(target, axis=1)\n\n# Split the data set into training and testing data sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21, stratify=y)\n# List of classification algorithms\nalgo_lst=['Logistic Regression',' K-Nearest Neighbors']\n\n# Initialize an empty list for the accuracy for each algorithm\naccuracy_lst=[]\n# Logistic regression\nlr = LogisticRegression(random_state=0)\nlr.fit(X_train,y_train)\ny_pred=lr.predict(X_test)\n\n# Get the accuracy score\nacc=accuracy_score(y_test, y_pred)\n\n# Append to the accuracy list\naccuracy_lst.append(acc)\n\nprint(\"[Logistic regression algorithm] accuracy_score: {:.3f}.\".format(acc))\ny_train_pred=lr.predict(X_train)\n\n# Get the accuracy score\naccc=accuracy_score(y_train, y_train_pred)\n\naccc\n# Create a k-NN classifier with 6 neighbors\nknn = KNeighborsClassifier(n_neighbors=6)\n\n# Fit the classifier to the data\nknn.fit(X_train,y_train)\n\n# Predict the labels for the training data X\ny_pred = knn.predict(X_test)\n\n# Get the accuracy score\nacc=accuracy_score(y_test, y_pred)\n\n# Append to the accuracy list\naccuracy_lst.append(acc)\n\nprint('[K-Nearest Neighbors (KNN)] accuracy_score:{:.3f} .'.format(acc))\ny_pred_train = knn.predict(X_train)\n\naccc=accuracy_score(y_train, y_pred_train)\naccc\n# Make a plot of the accuracy scores for different algorithms\n\n# Generate a list of ticks for y-axis\ny_ticks=np.arange(len(algo_lst))\n\n# Combine the list of algorithms and list of accuracy scores into a dataframe, sort the value based on accuracy score\ndf_acc=pd.DataFrame(list(zip(algo_lst, accuracy_lst)), columns=['Algorithm','Accuracy_Score']).sort_values(by=['Accuracy_Score'],ascending = True)\n\n\n\n# Make a plot\nax=df_acc.plot.barh('Algorithm', 'Accuracy_Score', align='center',legend=False,color='0.5')\n\n# Add the data label on to the plot\nfor i in ax.patches:\n    # get_width pulls left or right; get_y pushes up or down\n    ax.text(i.get_width()+0.02, i.get_y()+0.2, str(round(i.get_width(),2)), fontsize=10)\n\n# Set the limit, lables, ticks and title\nplt.xlim(0,1.1)\nplt.xlabel('Accuracy Score')\nplt.yticks(y_ticks, df_acc['Algorithm'], rotation=0)\nplt.title('Which algorithm is better?'.format(state, county))\n\nplt.show()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}